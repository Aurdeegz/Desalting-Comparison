{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4338222-f266-4699-8421-ff3e1577d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the module: helpers.general_helpers\n",
      "\n",
      "Loading the module: helpers.stats_helpers.py\n",
      "\n",
      "numpy        2.0.1\n",
      "scipy         1.14.0\n",
      "pandas        2.2.2\n",
      "\n",
      "Loading the module: helpers.argcheck_helpers\n",
      "\n",
      "Loading the module: helpers.mpl_plotting_helpers\n",
      "\n",
      "Loading the module: helpers.pandas_helpers\n",
      "\n",
      "pandas        2.2.2\n",
      "numpy         2.0.1\n",
      "\n",
      "matplotlib    3.9.1\n",
      "numpy         2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic data analysis for protein proteomics\n",
    "\n",
    "# First, import things\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "from venny4py.venny4py import venny4py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from math import log10, log2, ceil, floor, sqrt, log, e\n",
    "from missforest import MissForest\n",
    "import os\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from helpers import general_helpers as gh\n",
    "from helpers import stats_helpers as sh\n",
    "from helpers import proteomics_helpers as ph\n",
    "from helpers import mpl_plotting_helpers as mph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39bb76e5-ba05-4c2b-9c25-c4db55a8f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    transpose_file = gh.transpose(*file)\\n    file = [file[i] + log_data[i] for i in range(len(file))]\\n    file = [row + [safe_log2(sh.mean([row[i] for i in g])) for g in group_indices]\\n            for row in file]\\n    file = [row + [sh.standard_deviation([row[i] for i in log_g]) \\n                   for log_g in log_indices] for row in file]\\n    file = [row + [cv_perc(sh.standard_deviation([row[i] for i in log_g]))\\n                   for log_g in log_indices] for row in file]\\n    \\n    return file, transpose_file, log_data, group_indices, log_indices\\n\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptm_cols = {\"PG.Genes\":\"Gene Name\",\n",
    "            \"PTM.ModificationTitle\" : \"Modification Type\",\n",
    "            \"PTM.SiteAA\" : \"Amino acid\",\n",
    "            \"PTM.SiteLocation\" : \"Site\",\n",
    "            \"PTM.FlankingRegion\" : \"Flanking sequence\",\n",
    "             '01_0m_Waters_R1_sSH2_021725_103611.raw.PTM.Label-Free Quant' : \"Waters 0m R1\",\n",
    "             '02_0m_Waters_R2_sSH2_021725_125405.raw.PTM.Label-Free Quant' : \"Waters 0m R2\",\n",
    "             '03_0m_Waters_R3_sSH2_021725_151157.raw.PTM.Label-Free Quant' : \"Waters 0m R3\",\n",
    "             '04_0m_Waters_R4_sSH2_021725_172915.raw.PTM.Label-Free Quant' : \"Waters 0m R4\",\n",
    "             '06_2m_Waters_R1_sSH2_021725_213844.raw.PTM.Label-Free Quant' : \"Waters 2m R1\",\n",
    "             '07_2m_Waters_R2_sSH2_021725_235720.raw.PTM.Label-Free Quant' : \"Waters 2m R2\",\n",
    "             '08_2m_Waters_R3_sSH2_021825_021612.raw.PTM.Label-Free Quant' : \"Waters 2m R3\",\n",
    "             '09_2m_Waters_R4_sSH2_021825_043348.raw.PTM.Label-Free Quant' : \"Waters 2m R4\",\n",
    "             '11_0m_Thermo_R1_sSH2_021825_084238.raw.PTM.Label-Free Quant' : \"Thermo 0m R1\",\n",
    "             '12_0m_Thermo_R2_sSH2_021825_112430.raw.PTM.Label-Free Quant' : \"Thermo 0m R2\",\n",
    "             '13_0m_Thermo_R3_sSH2_021825_134217.raw.PTM.Label-Free Quant' : \"Thermo 0m R3\",\n",
    "             '14_0m_Thermo_R4_sSH2_021825_160027.raw.PTM.Label-Free Quant' : \"Thermo 0m R4\",\n",
    "             '16_2m_Thermo_R1_sSH2_021825_201012.raw.PTM.Label-Free Quant' : \"Thermo 2m R1\",\n",
    "             '17_2m_Thermo_R2_sSH2_021825_222835.raw.PTM.Label-Free Quant' : \"Thermo 2m R2\",\n",
    "             '18_2m_Thermo_R3_sSH2_021925_004649.raw.PTM.Label-Free Quant' : \"Thermo 2m R3\",\n",
    "             '19_2m_Thermo_R4_sSH2_021925_030422.raw.PTM.Label-Free Quant' : \"Thermo 2m R4\",\n",
    "             '21_0m_TECAN_R1_sSH2_021925_071305.raw.PTM.Label-Free Quant' : \"TECAN 0m R1\",\n",
    "             '22_0m_TECAN_R2_sSH2_021925_093037.raw.PTM.Label-Free Quant' : \"TECAN 0m R2\",\n",
    "             '23_0m_TECAN_R3_sSH2_021925_114900.raw.PTM.Label-Free Quant' : \"TECAN 0m R3\",\n",
    "             '24_0m_TECAN_R4_sSH2_021925_140639.raw.PTM.Label-Free Quant' : \"TECAN 0m R4\",\n",
    "             '26_2m_TECAN_R1_sSH2_021925_181659.raw.PTM.Label-Free Quant' : \"TECAN 2m R1\",\n",
    "             '27_2m_TECAN_R2_sSH2_021925_203504.raw.PTM.Label-Free Quant' : \"TECAN 2m R2\",\n",
    "             '28_2m_TECAN_R3_sSH2_021925_225228.raw.PTM.Label-Free Quant' : \"TECAN 2m R3\",\n",
    "             '29_2m_TECAN_R4_sSH2_022025_011034.raw.PTM.Label-Free Quant' : \"TECAN 2m R4\",\n",
    "             '31_0m_ProtiFi_R1_sSH2_022025_051949.raw.PTM.Label-Free Quant' : \"ProtiFi 0m R1\",\n",
    "             '32_0m_ProtiFi_R2_sSH2_022025_073707.raw.PTM.Label-Free Quant' : \"ProtiFi 0m R2\",\n",
    "             '33_0m_ProtiFi_R3_sSH2_022025_095447.raw.PTM.Label-Free Quant' : \"ProtiFi 0m R3\",\n",
    "             '34_0m_ProtiFi_R4_sSH2_022025_121303.raw.PTM.Label-Free Quant' : \"ProtiFi 0m R4\",\n",
    "             '36_2m_ProtiFi_R1_sSH2_022025_162403.raw.PTM.Label-Free Quant' : \"ProtiFi 2m R1\",\n",
    "             '37_2m_ProtiFi_R2_sSH2_022025_184130.raw.PTM.Label-Free Quant' : \"ProtiFi 2m R2\",\n",
    "             '38_2m_ProtiFi_R3_sSH2_022025_205945.raw.PTM.Label-Free Quant' : \"ProtiFi 2m R3\",\n",
    "             '39_2m_ProtiFi_R4_sSH2_022025_231759.raw.PTM.Label-Free Quant' : \"ProtiFi 2m R4\"}\n",
    "\n",
    "colours = [\"grey\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "files = [\"20250303_prepmethods_sSH2_ptm.csv\",\n",
    "         \"20250303_prepmethods_sSH2_peptides.csv\"]\n",
    "\n",
    "newheads = [\"Missing values\", \"median intensity\", \"tex formatted site\",\n",
    "           \"log2(Waters 0m R1)\", \"log2(Waters 0m R2)\", \"log2(Waters 0m R3)\", \"log2(Waters 0m R4)\", \n",
    "            \"log2(Waters 2m R1)\", \"log2(Waters 2m R2)\", \"log2(Waters 2m R3)\", \"log2(Waters 2m R4)\", \n",
    "             \"log2(Thermo 0m R1)\", \"log2(Thermo 0m R2)\", \"log2(Thermo 0m R3)\", \"log2(Thermo 0m R4)\", \n",
    "            \"log2(Thermo 2m R1)\", \"log2(Thermo 2m R2)\", \"log2(Thermo 2m R3)\", \"log2(Thermo 2m R4)\",\n",
    "             \"log2(TECAN 0m R1)\", \"log2(TECAN 0m R2)\", \"log2(TECAN 0m R3)\", \"log2(TECAN 0m R4)\", \n",
    "            \"log2(TECAN 2m R1)\", \"log2(TECAN 2m R2)\", \"log2(TECAN 2m R3)\", \"log2(TECAN 2m R4)\",\n",
    "             \"log2(ProtiFi 0m R1)\", \"log2(ProtiFi 0m R2)\", \"log2(ProtiFi 0m R3)\", \"log2(ProtiFi 0m R4)\", \n",
    "            \"log2(ProtiFi 2m R1)\", \"log2(ProtiFi 2m R2)\", \"log2(ProtiFi 2m R3)\", \"log2(ProtiFi 2m R4)\",\n",
    "             \"Waters 0m log(mean)\", \"Waters 2m log(mean)\",\n",
    "            \"Thermo 0m log(mean)\", \"Thermo 2m log(mean)\", \n",
    "            \"TECAN 0m log(mean)\", \"TECAN 2m log(mean)\", \n",
    "            \"ProtiFi 0m log(mean)\", \"ProtiFi 2m log(mean)\",\n",
    "             \"Waters 0m SD\", \"Waters 2m SD\", \n",
    "            \"Thermo 0m SD\", \"Thermo 2m SD\", \n",
    "            \"TECAN 0m SD\", \"TECAN 2m SD\", \n",
    "            \"ProtiFi 0m SD\", \"ProtiFi 2m SD\",\n",
    "             \"Waters 0m CV%\", \"Waters 2m CV%\",\n",
    "            \"Thermo 0m CV%\", \"Thermo 2m CV%\",\n",
    "            \"TECAN 0m CV%\", \"TECAN 2m CV%\",\n",
    "            \"ProtiFi 0m CV%\", \"ProtiFi 2m CV%\"]\n",
    "\n",
    "groups = [\"Waters 0m\", \"Waters 2m\",\n",
    "          \"Thermo 0m\", \"Thermo 2m\", \n",
    "          \"TECAN 0m\", \"TECAN 2m\", \n",
    "          \"ProtiFi 0m\", \"ProtiFi 2m\"]\n",
    "\n",
    "manufacturers = [\"Waters\", \"Thermo\", \"TECAN\", \"ProtiFi\"]\n",
    "\n",
    "def keep_first_row(file,\n",
    "                   keycol = \"flank\",\n",
    "                   heads = 0):\n",
    "    \"\"\"\n",
    "    has to be pre-sorted\n",
    "\n",
    "    trying to make a more efficient keep_first, we'll see\n",
    "    \"\"\"\n",
    "    keycol_ind = file[heads].index(keycol)\n",
    "    keepers = []\n",
    "    latest = None\n",
    "    i=0\n",
    "    for row in iter(file):\n",
    "        if row[keycol_ind] != latest:\n",
    "            latest = row[keycol_ind]\n",
    "            keepers.append(row)\n",
    "        if i % 50000 == 0:\n",
    "            print(i)\n",
    "        i+=1\n",
    "    return keepers\n",
    "\n",
    "def unique_ptm(file, \n",
    "               ptm_type = \"Phospho (STY)\",\n",
    "               ptm_col = \"Modification Type\",\n",
    "               aa = \"Amino acid\",\n",
    "               site = \"Site\",\n",
    "               gene = \"Gene Name\",\n",
    "               flank = \"Flanking sequence\"):\n",
    "    # First, get the column indices from the names\n",
    "    ptm_loc = file[0].index(ptm_col)\n",
    "    aa_loc = file[0].index(aa)\n",
    "    site_loc = file[0].index(site)\n",
    "    gene_loc = file[0].index(gene)\n",
    "    flank_loc = file[0].index(flank)\n",
    "    # Filter for only the PTM of interest\n",
    "    file = [file[0]] + [row for row in file if row[ptm_loc] == ptm_type]\n",
    "    # Make a string for the site\n",
    "    file = [file[0] + [\"Site String\"]] + [row + [f\"{row[gene_loc]}$^{{{row[aa_loc]}{int(row[site_loc])}}}$\"] for row in file[1:]]\n",
    "    # Filter for unique flanking sequences\n",
    "    file = [file[0]] + sorted(file[1:], key = lambda x: x[flank_loc])\n",
    "    file = keep_first_row(file, keycol = flank, heads = 0)\n",
    "    # Keep only tyrosine phosphorylated stuff\n",
    "    file = [file[0]] + [row for row in file if row[aa_loc] == \"Y\"]\n",
    "    # Return the file\n",
    "    return file\n",
    "\n",
    "def grab_sample_counts(file,\n",
    "                       group_sample_inds):\n",
    "    # First, transpose the file, assume it has headers\n",
    "    file_t = gh.transpose(*file[1:])\n",
    "    # Then, use the group indices to make some sums\n",
    "    counts = [[sum([1 for _ in file_t[j] if _ == _]) for j in group_sample_inds[i]]\n",
    "              for i in range(len(group_sample_inds))]\n",
    "    return counts\n",
    "\n",
    "def split_and_impute(file,\n",
    "                     glob_groups,\n",
    "                     header_row):\n",
    "    # First, bin the columns into dicts with the global groups\n",
    "    group_inds = [[i for i in range(len(header_row)) if g in header_row[i]]\n",
    "                  for g in glob_groups]\n",
    "    group_inds = [[0] + row for row in group_inds]\n",
    "    file_d = {glob_groups[i] : gh.transpose(*[file[j] for j in group_inds[i]]) for i in range(len(glob_groups))}\n",
    "    file_d = {key : [row for row in value if any([True for _ in row[1:] if _ == _])] for key, value in file_d.items()}\n",
    "    # Impute each matrix\n",
    "    file_i = {key : None for key, value in file_d.items()}\n",
    "    for key, value in file_d.items():\n",
    "        if os.path.exists(f\"tmp/{key}_imputed.csv\"):\n",
    "            file_i[key] = pd.read_csv(f\"tmp/{key}_imputed.csv\")\n",
    "        else:\n",
    "            data = pd.DataFrame([row[1:] for row in value], \n",
    "                                index = gh.transpose(*value)[0]).transpose()\n",
    "            data.to_csv(f\"tmp/{key}_no_impute.csv\")\n",
    "            imputer = MissForest()\n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "            imputed_data.to_csv(f\"tmp/{key}_imputed.csv\")\n",
    "            file_i[key] = imputed_data\n",
    "    return file_i\n",
    "\n",
    "def read_file(filename,\n",
    "              rename_columns,\n",
    "              group_labels,\n",
    "              new_heads,\n",
    "              glob_group = manufacturers,\n",
    "              protein = True,\n",
    "              sort_head = \"Gene Name\",\n",
    "              ptm = False,\n",
    "              ptm_kwargs = {\"ptm_type\" : \"Phospho (STY)\",\n",
    "                            \"ptm_col\" : \"Modification Type\",\n",
    "                            \"aa\" : \"Amino acid\",\n",
    "                            \"site\" : \"Site\",\n",
    "                            \"gene\" : \"Gene Name\",\n",
    "                            \"flank\" : \"Flanking sequence\"}):\n",
    "    file = pd.read_csv(filename)\n",
    "    file = file.rename(columns = rename_columns)[list(rename_columns.values())]\n",
    "    original_heads = list(file.columns.values)\n",
    "    group_indices = [[i for i in range(len(file.columns)) if g in file.columns[i]]\n",
    "                     for g in group_labels]\n",
    "    data_range = [min(gh.unpack_list(group_indices)), max(gh.unpack_list(group_indices))]\n",
    "    # Then continue as normal\n",
    "    log_indices = [[i + 3 + len(original_heads) for i in gind] for gind in group_indices]\n",
    "    file = [list(row) for row in file.to_numpy()]\n",
    "    file = [gh.transform_values(row, transform = float) for row in file]\n",
    "    # because there are strange characters in the gene names\n",
    "    file = [row for row in file if type(row[0]) == str]\n",
    "    # Calculate missing values across a row\n",
    "    file = [row + [sum([1 for _ in row[data_range[0]:data_range[1]] if _ == _])] for row in file]\n",
    "    # Calculate highest median intensity\n",
    "    file = [row + [sh.median(row[data_range[0]:data_range[1]])] for row in file]\n",
    "    # Sort the file based on those two criteria and the sort column\n",
    "    file = sorted(file, key = lambda x: (x[original_heads.index(sort_head)], x[-2], -x[-1]))\n",
    "    if ptm:\n",
    "        file = unique_ptm([original_heads + new_heads] + file, **ptm_kwargs)\n",
    "    # Add a function to grab the sample counts\n",
    "    persamp_counts = grab_sample_counts(file, \n",
    "                                        group_indices)\n",
    "    # Then log stransform\n",
    "    log_data = [[safe_log2(item) for item in row[min(gh.unpack_list(group_indices)):max(gh.unpack_list(group_indices))+1]]\n",
    "                for row in file[1:]]\n",
    "    # and split/input the logged data\n",
    "    imputable_data = [[file[i+1][original_heads.index(sort_head)]]+log_data[i] for i in range(len(log_data))]\n",
    "    log_i = split_and_impute(gh.transpose(*imputable_data), \n",
    "                            glob_group,\n",
    "                            header_row = [sort_head] + [head for head in new_heads if \"log2(\" in head])\n",
    "    return log_i, persamp_counts\n",
    "\"\"\"\n",
    "    transpose_file = gh.transpose(*file)\n",
    "    file = [file[i] + log_data[i] for i in range(len(file))]\n",
    "    file = [row + [safe_log2(sh.mean([row[i] for i in g])) for g in group_indices]\n",
    "            for row in file]\n",
    "    file = [row + [sh.standard_deviation([row[i] for i in log_g]) \n",
    "                   for log_g in log_indices] for row in file]\n",
    "    file = [row + [cv_perc(sh.standard_deviation([row[i] for i in log_g]))\n",
    "                   for log_g in log_indices] for row in file]\n",
    "    \n",
    "    return file, transpose_file, log_data, group_indices, log_indices\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c3be898-3ee7-4ff1-a1cf-c7ccf9d57b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8461/1643324594.py:166: DtypeWarning: Columns (143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file = pd.read_csv(filename)\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.45s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.34s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.81s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.56s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.98s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.13s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:20<00:00, 76.12s/it]\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "/home/aurorathaho/python312/lib/python3.12/site-packages/missforest/missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:21<00:00, 28.27s/it]\n"
     ]
    }
   ],
   "source": [
    "ptm = read_file(files[0],\n",
    "                ptm_cols,\n",
    "                groups,\n",
    "                newheads,\n",
    "                ptm= True,\n",
    "                sort_head = \"Flanking sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879008c2-fb82-44ef-b2f9-847522cd8775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f629fadb-35a5-4f47-a023-6f4447a83467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135, 121, 98, 154], [167, 169, 144, 196], [41, 67, 64, 51], [134, 127, 82, 149], [32, 35, 26, 76], [64, 88, 51, 123], [527, 431, 310, 306], [557, 418, 465, 617]]\n"
     ]
    }
   ],
   "source": [
    "print(ptm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d25b7-8518-4567-9154-2f876612dcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdd344-a0cb-461e-85a7-1e8f6844256c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
